[文章](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-reasoning-llms)
什么是推理模型：
推理模型在回答问题前尝试将问题拆解为更小步骤然后再得出结论
为什么要创建推理模型？
1. 什么是训练时间算力（ train-time compute ）？
   之前，提升预训练期间 LLM 的性能一般是提升下面三个条件的大小：模型（参数），数据集（ token ）和 算力（ FLOPs ）。三个合在一起叫做 __训练时间算力（ train-time compute ）__。它包含训练和微调所需的算力。
2. 规模定律（这个是 train-time compute 的规模定律）
   scaling law 是研究模型性能和模型规模（算力，数据大小和模型大小）。这些定律通常是 log-log 规模（这会得到一个直线），为了显示算力的大规模增长。最常见定律的是“ Kaplan ”和“ Chinchilla ” scaling law 。这些定律表示模型的性能随着更多的算力， tokens 和参数增长。 Kaplan scaling law 表示扩大模型大小通常比扩大数据更有效（固定算力条件）。Chinchilla scaling law 认为模型大小和数据同等重要。不过在2024年各个模型使用的数据大小和模型参数稳定增长但是收益不大。因此，提出了撞墙的问题。
3. 什么是 测试时间算力（ test-time compute ）？
   由于增长 train-time compute 很昂贵， test-time compute 作为替代被考虑。它不再持续增加预训练的预算，而是允许模型在推论（ inference ）期间思考更长时间。相比于非推理模型跳过推理步骤直接输出答案，推理模型会使用更多的 token 通过一个系统的思考过程推理自己的答案。这个想法是 LLM 必须要花费资源（ 比如 VRAM 算力 ）生成答案，然而，如果所有算力都用来生成答案，会有点低效。相反，通过预先创建更多包含额外信息，关系和新想法的 tokens ，模型花费更多算力生成最终答案。（更多 token = 更多算力 = 更多时间思考 = 更好的性能）（我理解这句话的意思是，模型不在投入算力直接得出结果，而是投入算力得到中间步骤，然后推出结果）
4. 规模定律（这个是 test-time compute 的规模定律）
   相对于 train-time compute ， test-time compute 的规模定律相对较新。 OpenAI 展示了 test-time compute 可能和 train-time compute 的趋势一样。《 Scaling Scaling Laws with Board Games 》这个论文展示了二者紧紧相关。由于 test-time compute 和 train-time compute 很像，出现了一种范式转移，转向了使用更多 test-time compute 的推理模型。相对于纯专注于 train-time compute（预训练和微调），推理模型会在推论和训练之间平衡。 Test-time compute 甚至会在思考长度上扩展。
5. Test-time compute 的分类
   推理模型除了简单的“更长的”思考还有很多技术。 test-time compute 包含很多内容（ COT ，修改答案，回溯，采样等等），大概分成两类：
   1. search against verifiers ，包括采样生成和最佳答案选择（ sampling generations and selecting the best answer ）。这一类关注输出模型生成答案，然后验证器验证答案质量。本文讨论两类验证器：输出奖励模型（ outcome reward model ORM ）和过程奖励模型（ process reward model PRM ）。前者仅判断输出而不关注底层的处理过程，后者还会判断导致结果的过程（推理）。验证器涉及到两个步骤：首先，创建推理过程和结果的多个采样；然后，验证器（奖励模型）对生成输出打分。这个验证器也是一个 LLM ，会针对输出（ ORM ）或过程 （ PRM ） 微调。使用验证器的好处是不需要重新训练或者微调回答问题的 LLM 。
      1. 最直接的方式不是使用奖励模型或者验证器，而是多数选举（ majority voting ）。让模型生成多个答案（都包含推理步骤），而生成最多的通常就是最后的答案。这个方法叫做自洽（ self-consistency ），强调生成多个答案和推理步骤的重要性。
      2. 最佳 N 个采样（ best-of-N sample ）是第一个涉及到验证器的方法。它会生成 N 个例子然后使用验证器（ ORM ）判断答案：首先， LLM （通常叫做 Proposer ）生成多份答案（要么使用高的温度，要么是多个温度，温度是深度学习中的一个概念，是模型的一个超参数，简单来说，温度越高越随机）。然后，每个答案都会通过 ORM ， ORM 会对答案质量打分，选择最高分。也可能会使用 PRM 判断每个推理过程每一步推理的质量， PRM 会选择最高权重的结果。还可以结合这两类验证器，可以给每个 RM 的候选结果赋予权重，并选择最大的总权重结果。这叫做有权重的最佳 N 采样（ weighted best-of-N samples ）。
      3. 生成答案和中间过程的步骤还可以使用束搜索（ beam search ）扩展。使用束搜索，多个推理步骤被采样并通过 PRM 判断（类似于思考树 tree of thought ），top3 beams （最佳得分的路径） 会在整个过程中被跟踪。这一方法允许快速停止那些没有结果的推理路径（ PRM 打分低的路径 ）。最终结果通过 best-of-N 赋予权重，最终选择最高得分的答案。
      4. 让树搜索高效的一个方式是蒙特卡洛树搜索（ MCTS ）。包含四个步骤： selection （根据预定义的公式选择一个叶子）， expand （创建额外的节点）， rollout （随机创建新的节点直到遇到重点）， backprop （基于输出更新夫节点，反向传播）。这些步骤的主要目标是为了在保持扩展最佳推理步骤的情况下探索其他路径。这是在 探索（ exploration ） 和 利用（ exploitation ） 之间保持平衡，一个关于节点如何选择和打分的公式：$selection\,score = \frac{total\,node\,reward}{\#\,of\,node\,visits} + C \sqrt{\frac{\#\,of\,parent\,node\,visits}{\#\,of\,node\,visits}}$。这样，当我们选择一个新的推理步骤去扩展，就不会离最佳评分路径太远。使用这类公式，我们可以从选择一个节点（推理步骤）开始，然后通过生成新的推理步骤扩展它。这可以通过合理的高温度和不同温度值实现。这些扩展的推理步骤中的一个被选择并 roll out 多次知道它达到几个结果。这些路径可以通过处理过程（ PRM ），奖励（ ORM ）来判断，或者结合起来。夫节点的得分通过反向传播被更新，然后可以再次执行选择重启这个过程。
   2. modifying proposal distribution （ 训练过的思考过程 ）关注于输入（微调模型在回答前学习推理）。换句话说，从完成/思考/ tokens 取样的分布被修改了（ the distribution from which completions/thoughts/tokens are sampled is modified ）。加上我们有一个问题以及从我们采样 token 得来的分布。通常我们会得分最高的 token 。现在，不在贪婪的选择最高得分的 token ，而是选择会导致推理过程的 token ，使得倾向于得到改进答案的结果。因此，我们修改提案分布（ token 的概率分布），本质上就是让模型对分布重排序使得“推理” token 被更平凡的选择。修改 proposal distribution 的方式分为两类：
      1. 通过提示工程更新提示：使用提示修改 proposal distribution ，我们需要提供示例给模型（上下文学习 in-context learning ），让模型遵循类似推理的步骤。这样，会让 LLM 倾向于在回答问题前分解过程，从而修改 proposal distribution 。这一过程是静态的，模型内在没有学习如何遵循这一过程。如果一个模型从一个错误的推理过程开始，就会倾向于保持而不是修改它。 
      2. 训练模型关注推理 token/步骤：可以通过训练模型，奖励它生成推理步骤，训练它学会“推理”。这一般涉及到一系列推理数据和奖励某些行为的强化学习。 STaR （ Self-Taught Reasoner ）最常用。它使用 LLM 生成自己的推理数据，作为微调这个模型的输入。第一步，生成推理步骤和答案，如果答案正确，将推理和答案加入到三元组（问题， 推理，答案）训练数据集。这个数据用在执行对这个模型的有监督微调（ supervised fine-tuning ）。如果模型生成了错误答案，就提供一个提示（正确答案）并要求模型推理为什么这个答案正确。最终的结果被加入到同样的三元训练数据中用来执行对这个模型的有监督微调。一个关键点是，要显式的训练模型遵循我们展示给他的推理步骤，通过有监督微调决定推理步骤应该是怎样的。这整个流水线本质上生成了合成训练示例。使用合成训练示例对于在其他模型中蒸馏这个推理过程是非常好的方法。
6. Deepseek-R1
   1. 最开始是 DeepSeek-R1 Zero 这个实验模型，从 DeepSeek-V3-Base 开始，不是通过在一系列推理数据上进行有监督微调，作者仅使用强化学习（ reinforcement learning RL ）启用推理行为。作者在模型流水线中加入了提示词，大意是要求模型回答用户问题前先考虑推理过程，并用`<think></think>`包裹推理步骤`<answer></answer>`包裹答案。注意这要求推理过程应该被`<think>`标签包裹，但没有指定推理过程应该是什么样的。在强化学习阶段，创建了两个特殊的基于规则的奖励：精确奖励（ accuracy rewards ）通过测试输出奖励答案以及格式奖励（ format rewards ）奖励使用了`<think>`和`<answer>`的标签。用于这一过程的 RL 算法叫做 Group Relative Policy Optimization （ GRPO ）。它的原理是让所有导致正确或者不正确的答案的选择更可能或者更不可能。这些选择可以是 token 集合也可以是推理步骤。示例中没有给出`<think>`过程应该是什么样子的，通过提供这些和思考链（ Chain-of-Thought ）行为相关的间接奖励，模型自己学习到了思考越长，推理步骤越复杂，结果就觉可能正确。但是这里显著的缺点是糟糕的可读性以及倾向于混合语言。
   2. R1 模型的创建使用下面五个步骤：冷启动，面向推理的强化学习，拒绝采样，有监督微调和针对所有场景的强化学习。第一步，使用少量高质量的推理数据集微调了 DeepSeek-V3-Base 。这么做是为了解决冷启动问题导致的可读性差。在第二步中，使用了和训练 DeepSeek-V3-Zero 的 类似的 RL 过程训练上一步得到的模型，但是为了确保目标语言的一致，添加了额外的奖励机制。上一步的结果模型用来生成合成推理数据，为了后续阶段的有监督微调。通过拒绝采样（ 基于规则的奖励 ）和奖励模型（ DeepSeek-V3-Base ）得到了60w的高质量推理示例。进一步的，使用训练 DeepSeek-V3 的部分数据以及由 V3 生成的数据共20w，一共80w的数据，执行对 V3-Base 的有监督微调得到 R1 。第五步，使用了类似用于 R1-Zero 的方式对上一步得到的模型执行了基于 RL 的训练。 为了对齐人类偏好，还增加了奖励信号专注于有用和无害，以及，为了阻止可读性问题，还要求模型总结推理过程。因此， R1 是 V3-Base 的通过有监督微调和强化学习得到的微调结果。
7. 使用 R1 蒸馏推理
   R1 基本不可能在消费者硬件上运行。不过可以蒸馏 R1 的推理能力到其他模型，比如 Qwen-32B 。他们使用 R1 作为教师模型，其他小模型作为学生模型。所有这些模型都使用同一个提示词并要求生成 token 的概率分布。在训练过程中，学生将尝试紧跟教师的分布。这也使用了上述 80w 的高质量示例。这样，蒸馏后的模型不仅从数据中学习，还学习到了 R1 回答问题的方式。
8. R1 也尝试了 PRM 和 MCTS ，但是没有成功。 MCTS 的问题是搜索空间很大，因此不得不限制节点扩展。而且本身细粒度的奖励模型就很难。对于最佳 N 的 PRM 技术，他们遇到的问题是为了持续重新训练奖励模型以避免奖励劫持（ reward-hacking ）导致的算力开销。