## log结构文件系统
在二十世纪90年代早期，在Berkeley由John Ousterhout和研究生Mendel Rosenblum领导的一个研究小组开发了一个新的文件系统：log结构文件系统。这样做的动机是基于如下观察：
* __系统内存在增长：__ 随着内存越大，可以在内存中缓存的数据越多。越多的数据被缓存，硬盘流量的增长逐渐是有写操作组成的，因为读是缓存提供服务的。因此，文件系统性能大部分是由它的写操作性能决定了。
* __随机I/O性能和顺序I/O性能之间有巨大的间隙：__ 硬盘驱动传输带宽这些年增长迅速；更多的bit被封装到了驱动的表面，访问这些bit时带宽增加了。寻道和旋转延时开销很大，然而，下降的很慢；让便宜小巧的马更快的旋转磁盘或者更快的移动硬盘臂具有挑战性。因此，如果你有能力可以顺序使用硬盘，相比于那些导致寻道或和旋转的方式，你就获取了可测量的性能提升。
* __已存在的文件系统在很多常见工作集上工作的很糟糕：__ 例如，FFS创建一个block大小的文件需要执行大量的写操作；一次写新的inode，一次更新inode位图，一次写包含文件的目录数据block，一次更新这个目录的inode，一次写这个新文件的新数据block，一次写数据位图标识这个数据块被分配了。因此，尽管FFS把所有这些blocks都放到同一个blcok组里，FFS导致很多小的寻道以及后续的旋转延时从而性能相对峰值顺序带宽急剧下降。
* __文件系统对RAID无意识(no RAID-aware)：__ 例如，RAID-4和RAID-5都有 __小尺寸写问题(small-write problem)__ ：一个逻辑写单个block导致4次物理I/O的发生。已存在的文件系统没有尝试避免这个最糟糕的RAID写行为情况。

一个理想的文件系统应该关注写性能，并尝试利用硬盘的顺序写入带宽。更进一步，它在常见的工作集上工作的很好，不仅仅是写入数据，还有频繁的更新硬盘元数据结构。最后，它在RAID上工作的要跟在单个硬盘上一样好。
>#### tip：细节很重要
>所有有意思的系统都是由少数通用概念和很多细节组成的。有时候，当你学习这些系统时，你认为你自己“我知道了大体的概念了，剩下的都是细节了”，你用这种方式只学习了一半关于事情如何真正工作的。不要这样做！很多次，细节是关键。正如我们在LFS中看到的，大体的概念很容易理解，但是真正构建一个可以工作的系统，你需要考虑说有麻烦的情况。

Rosenblum和Ousterhout引入的新型文件系统叫做  __LFS__，是 __Log结构文件系统(Log-structured File System)__ 的简称。当写入到硬盘时，LFS首先在内存中的 __段(segment)__ 缓冲所有更新(包括元数据!)；当这个段满了，就把它以长的，顺序的传输方式写入到硬盘中没有使用的部分中(it is written to disk in one long ,sequential transfer to an unused part of the disk)。LFS绝不会覆写存在的数据，总是把段写入到空闲的位置。由于段很大，硬盘(或者RAID)就被高效的使用了，文件系统的性能就接近它的峰值。
>#### 症结：如果让所有的写都是顺序写？
>文件系统如何把所有写操作都转换为顺序写？对于读，这个任务是不可能的，因为需要读取的block可能在硬盘的任意位置。对于写，文件系统总是有选择的，这中选择就是我们要揭示的。

### 43.1 顺序写入硬盘
现在我们有了第一个挑战：我们要怎么做才能把所有对文件系统状态的更新转换为一系列对硬盘的顺序写操作？为了更好的理解这个，让我们用一个简单的例子。考虑我们正在写入把数据block _D_ 写入到硬盘。写入数据到硬盘可能会导致如下硬盘布局，_D_ 写入到了硬盘地址 _A0_：

![show43_1.png "写入了一个数据block的硬盘布局"](show43_1.png "写入了一个数据block的硬盘布局")

然而，当用户写入数据block时，不仅仅写入了数据，还有其它元数据需要更新。在这个情况下，我们把文件的 __inode(I)__ 也写入到硬盘，并让它指向数据block _D_。当写入到硬盘后，数据block和inode看起来如下(注意inode和数据block看起来一样大，通常不是这样子的，在大多数系统中，数据blocks的大小是4KB，而inode要小的多，大概128字节):

![show43_2.png "写入了数据block和inode的硬盘布局"](show43_2.png "写入了数据block和inode的硬盘布局")

基本的想法，顺序的写入所有更新到硬盘(例如，数据blocks，inodes等等)，是LFS的中心思想。如果你理解了这个，你就有了基本的概念。但是就想所有复杂的系统那样，细节是魔鬼。

### 43.2 高效的顺序写入
不幸的是，只是顺序写入硬盘不足以保证高效的写入。例如，考虑我们在时刻$T$向地址$A$写入了单个block。然后我们等一小会，在硬盘地址$A+1$(按照顺序是下一个block的地址)写入数据，时刻是$T+\delta$。在第一次和第二次写之间，不幸的是，硬盘已经旋转了；当你发起第二次写，它最长将等待几乎一个旋转延时后才能被提交(具体地说，如果旋转花费了时间是 $T_{rotation}$，那么硬盘会等待$T_{rotation}-\delta$时间才能提交第二次对硬盘盘面的写操作)。因此你会想要看到简单的顺序写入硬盘不足以达到峰值性能；相反，你需要对硬盘发起大量 _连续(contiguous)_ 写操作(或者一个大的写操作)才能达成好的写入性能。

为了达到这个结果，LFS使用一个古老的技术叫做 __写缓冲__[<sup id="content1">__1__</sup>](#1)。在写入硬盘前，LFS在内存中保持更新的轨迹；当它接收到了足够多数量的更新，它一次把这些都写入到硬盘中，从而确保高效的使用硬盘。

LFS一次写入的大chunk更新叫做 __段(segment)__。尽管这个词语在计算机系统中被过度使用了，这里它只表示LFS用来分组写操作的足够大的chunk。因此，当写入硬盘时，LFS缓冲更新到内存段中，然后在一次性把这个段都写入到硬盘中。只要这些段足够大，这些写操作就会很高效。

这有个例子，LFS在一个小的段中缓冲了两个更新操作集；实际的段要更大些(几MB)。第一个更新是写入到文件`j`四个block；第二个更新是对文件`k`增加一个block。LFS然后一次性提交包含这个七个block的整个段给硬盘。这些block在硬盘上的最终布局如下：

![show43_3.png "带有写缓冲后写操作在硬盘的布局"](show43_3.png "带有写缓冲后写操作在硬盘的布局")

### 43.3 缓冲多少？
这引起了如下问题：在写入到硬盘前，LFS要缓冲多少更新？当然，答案是依赖硬盘自身，特别是和传输率相比定位负载是多高；类似的分析请查阅FFS章节。

例如，假设每次写入前定位(例如旋转和寻道负载)花费大约$T_{position}$秒。进一步假设，硬盘传输率是$R_{peak}$MB/s。在运行这样的硬盘上时，LFS写入前应该缓冲多少数据？

考虑这个问题的方式是每次你写入数据时，你都会花费固定的定位开销负载。因此，你需要写入多少来 __摊销(amortize)__ 这个开销？你写入的越多，(显然)越好，越接近你要达到的峰值带宽。

为了获取一个具体的结果，我们假设写入 $D$MB数据，写入这块(chunck)数据的时间($T_{write}$)是定位时间$T_{position}$加上传输$D$($\frac{D}{R_{peak}}$)的时间:

$T_{write} = T_{position} + \frac{D}{R_{peak}} (43.1)$

因此，实际写入速率($R_{effective}$)，写入数据量除以写入所需总时间：

$R_{effective} = \frac{D}{T_{write}} = \frac{D}{T_{position}+\frac{D}{R_{peak}}} (43.2)$

我们感兴趣的是让实际速率($R_{effective}$)接近峰值速率。具体的说，我们想要实际速率达到峰值速率的某个$F$倍，这里$0<F<1$(一个典型的$F$可能是0.9，或者是峰值速率的90%)。用数学的形式，这意味着我们想要 $R_{effective} = F \times R_{peak}$

此时，我们可以解出D:

$R_{effective} = \frac{D}{T_{position}+\frac{D}{R_{peak}}} = F \times R_{peak} (43.3)$

$D = F\times $R_{peak}\times(T_{position} + \frac{D}{R_{peak}}) (43.4)$

$D = (F\times R_{peak} \times T_{position}) + (F\times R_{peak} \times \frac{D}{R_{peak}}) (43.5)$

$D = \frac{F}{1-F}\times R_{peak}\times T_{position} (43.6)$

让我们做个例子，硬盘的定位时间是10ms，峰值传输速率是100MB/s；假设我们想要实际带宽是峰值的90%($F=0.9$)。在这个情况下，$D=\frac{0.9}{0.1}\times 100MB/s \times 0.01 seconds = 9MB$。尝试用不同的值来计算下为了达到峰值带宽我们需要缓冲多少数据。达到95%需要多少，达到99%需要多少？

### 43.4 问题：找到inodes
为了理解我们要怎么找到LFS的inode，我们先简单的回顾一下在一个典型的UNIX文件系统中怎么查找inode。在像FFS这样典型的文件系统中，或者甚至是旧的UNIX文件系统，查找inode很容易的，因为它们被组成一个数组并放置在硬盘上一个固定位置。

例如，旧的UNIX文件系统吧所有的inode都放在硬盘的固定位置上。因此，给定一个inode号和开始地址，为了找到一个特定的inode，你可以通过用inode大小乘以inode号在加上这个硬盘上的数组开始地址，就可以简单的精确计算出硬盘地址了；基于数组的索引，给了inode号，很快并且很直接。

在LFS中，就比较麻烦了，为什么？我们把inode散布到整个硬盘上了。更糟糕的是，我们永远不会原地重写，因此，一个inode的最新版本(我们需要的版本)会一直移动。

### 43.5 通过间接的解决方案：inode映射
为了解决这个问题，LFS的设计者通过一个叫做 __inode映射(inode map imap)__ 在inode号和inode之间引入了一个 __间接层级(level of indirection)__。这个imap是一个数据结构，输入时inode号，输出是inode最新版本的硬盘地址。


[<sup id="1">1</sup>](#content1)事实上，对这个想法很难找到好的引用，因为它在计算历史上很早的时候就被很多系统发明了。对于写缓冲行为的一个研究，请查阅Solworth和Orji的论文，了解它前在的伤害，查阅Mogul。