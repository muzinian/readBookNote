## 崩溃一致性：FSCK和日志(Crash Consistency:FSCK and Journaling)
目前为止我们看到，文件系统通过管理一系列数据结构实现期望的抽象：文件，目录以及其它所有为了支撑这些我们希望从文件系统得到的基本抽象所必须的元数据。不想大多数数据结构(例如，那些在一个运行中程序内存中找到的)，文件系统数据结构必须是 __持久化的(persist)__，例如，它们必须可以在长途运输中存活，存储在那些掉电时也可以保存数据(例如硬盘或者基于闪存的SSDs)。

文件系统面临的一个主要挑战是如何在 __存在掉电__ 或者 __系统崩溃__ 的情况下更新持久化数据结构。具体的说，如果，在更新硬盘结构的过程中，某人被电源线绊倒导致机器掉电或者操作系统遇到了bug崩溃了会发生什么？由于掉电或者崩溃，更新一个持久化数据结构可能会很麻烦，从而导致在文件系统实现中出现了新的有意思的问题，也就是 __崩溃一致性问题(crash-consistency problem)__？

这个问题很容易理解。考虑为了完成一个特定操作，你要更新两个硬盘数据结构，A和B。因为一次只能服务一个请求，这些请求中会有一个先到达服务器(不是A就是B)。如果在一个写操作完成后系统崩溃或者掉电，硬盘数据结构就会出现 __不一致(inconsistent)__ 的状态。因此，我们这就有了一个文件系统必须要解决的问题：
>#### 症结：如何更新硬盘无视崩溃
>在任意写操作之间系统可能会崩溃或者掉电，从而硬盘状态可能只有部分被更新。崩溃之后，系统启动，想要再次挂载文件系统(为了访问文件等等)。由于崩溃可能发生在任意时刻，我们要怎么确保文件系统保持硬盘影像(image)是一个合理状态？

本章，我们会详细的描述这个问题，然后学些文件系统解决这个问题的方法。我们首先从旧文件系统采用的方式开始，即 __fsck__ 或者 __文件系统检查(file system checker)__。然后我们把注意力转向到其它方式，也就是 __日志(journaling)__(也叫 __先写日志(write-ahead logging)__)，这是一个对每次写操作会稍微增加一点负载，但可以快速从崩溃或者掉电中恢复的技术。我们还会讨论日志的基本机制，包括Linux ext3(一个相对现代的日志文件系统)一些小小的不同倾向的实现。

### 42.1 一个更细节的例子
先放下对日志的研究，我们看一个例子。我们需要使用一个以某种方式更新硬盘数据结构的 __工作集__。假设这里的工作集很简单：对一个存在文件追加单个数据block。完成这个任务包括打开文件，调用`lseek()`移动文件偏移量到文件末尾然后发起一个4KB写操作到文件，最后关闭这个文件。

让我们假设我们在硬盘上使用标准的简单文件系统结构，类似我们之间看到过的文件系统。这个小例子包括一个 __inode位图(8bit，每个inode一个)__，一个__data位图__(8bit，每个数据block一个)，以及数据block(8个，序号从0到7)。下图是这个文件系统的示例：

![show42_1.png "文件系统示例"](show42_1.png "文件系统示例")

如果你看了图片中的结构，你可以看到已经分配了一个inode(inode号是2)，它在inode位图被标记了并分配了一个数据block(数据block 4)，同样在数据位图中被标记了。这个inode标记为I\[v1\]，表示这个inode的第一个版本；它间很快被更新(由于上面描述的工作集)。

让我们看看这个简化版inode内部。在I\[v1\]内部，我们看到：
```txt
owner : remzi
permissions : read-write
size : 1
pointer : 4
pointer : null
pointer : null
pointer : null
```
这个简化版inode，文件`size`是1(他分配了一个block)，第一个直接指针指向block 4(文件的第一个数据block)，其它三个直接指针都被设置为`null`(标识他们没有被使用)。当然，真实的inode有更多的域；查看之前的章节了解更多。

当我们追加内容到这个文件，我们增加了一个新的数据block给他，这样必须要更新三个硬盘结构：inode(指向新的block并记录由于追加了内容新的更大的尺寸)，新的数据block Db，以及新版本的数据位图(叫做B\[v2\])指示新数据block已经分配了。

因此，在这个系统的内存中，我们有三个block要写入到硬盘中。更新过的inode(inode版本2，简称I\[v2\])现在看起来如下：
```txt
owner : remzi
permissions : read-write
size : 2
pointer : 4
pointer : 5
pointer : null
pointer : null
```
更新过的数据位图(B\[v2\])像这样：`00001100`。最后，还有数据block(Db)，包含了用户追加到文件中的内容。

我们看到的文件系统最终硬盘映像如下：

![show42_2.png "第一次更新后的文件系统"](show42_2.png "第一次更新后的文件系统")

为了完成这种变换，文件系统必须要执行三次单独的硬盘写操作，一次是写inode(I\[v2\])，一次是写位图(B\[v2\])，一次是数据block(Db)。注意，这些写操作通常不会是用户发起`write()`系统调用时立即发生；而是，脏inode，位图，以及新数据先在主内存(在 __页缓存(page cache)__ 或者 __缓冲缓存(buffer cache)__ 中)待一段时间；然后，当文件系统最后决定把他们写入到硬盘(5-30秒后)，文件系统将会发起向硬盘发起必要的写请求。不幸的是，在这期间可能会发生崩溃从而干扰这些对硬盘的更新。具体的，如果崩溃发生时已经完成了一次或者两次写操作，但并没有全部完成，文件系统就处于一个有趣的状态了。

#### 崩溃场景
为了更好的理解问题，让我们看看一些崩溃例子。考虑只有一次写成功了；那么就会有三种可能，罗列如下：
* __只有数据block(Db)被写入到硬盘__。在这种情况下，数据在硬盘上，但是没有指向它的inode，也没有位图宣称这个block被分配了。因此，就好像这个写就没有发生过一样。这种情况不是一个问题，从文件系统崩溃一致性的角度来看[<sup id="content1">1</sup>](#1)。
* __只有对inode(I\[v2\])的更新写入了硬盘__。在这种情况下，indoe指向了Db应该写入到的硬盘地址(5)，但是Db还没有写入。因此，如果我们信任这个指针，我们就会从硬盘中读取到 __垃圾(garbage)__ 数据(硬盘地址5的旧数据)。
  进一步，我们有一个新的问题，叫做 __文件系统不一致(file system inconsistency)__。硬盘位图告诉我们数据block 5没有分配，但是inode说已经分配了。位图和inode之间的差异就是文件系统数据结构的一种不一致；为了使用文件系统，我们必须要以某种方式解决问题(下面会有)。
* __只有对位图(B\[v2\])的更新写入了硬盘__。在这种情况下，位图标识了数据block 5被分配了，但是没有inode指向它。因此文件系统再一次不一致了；如果不解决，这个写就会导致 __空间泄露(space leak)__，因为block 5将永远不会被文件系统使用。

还有另外三种崩溃场景会发生在这次尝试对硬盘写入三个块的操作中。在这些例子中，成功的写入了两次，最后一次失败了：
*  __inode(I\[v2\])和位图(B\[v2\])写入到硬盘，但是数据没有(Db)__。在这种情况下，文件系统元数据完全一致：inode有一个指向数据block 5的指针，位图指示了这个5是使用中的，从文件系统元数据的角度来看，所有看起来都OK。但是有一个问题：5是垃圾数据。
*  __inode(I\[v2\])和数据block(Db)写入了，但位图(B\[v2\])没有__。在这种情况下，inode指向硬盘上正确的数据，但是在inode和旧版本的位图(B1)之间又一次出现了不一致。因此，我们再一次需要在使用文件系统前解决这个问题。
*  __位图(B\[v2\])和数据block(Db)写入了，但是inode(I\[v2\])没有__。这种情况下，我们又一次遇到了inode和数据位图之间的不一致。然而，尽管block写入了，位图也指示了它是使用中，我们还是没有办法找到它属于哪个文件，因为没有inode指向这个文件。

#### 崩溃一致性问题
希望从这些崩溃场景中，你可以看到由于崩溃我们的硬盘文件系统映像(file system image)会发生多少问题：我们在文件系统数据结构中会有不一致；我们会有空间泄露；我们会返回给用户垃圾数据；等等。我们希望的是可以 __原子地(atomically)__ 把文件系统从一个一致性状态移动(追加内容到文件前)到另一个状态(例如，inode，位图，以及新的数据block写入到了硬盘)。不幸的是，由于硬盘一次只提交一次写操作，我们没法很简单的做到这些，崩溃或者掉电可能在这几个更新之间发生。我们称这种一般性问题为 __崩溃一致性问题(crash-consistency problem)__(我们也会称之为 __一致性更新问题(consistent-update problem)__)。

### 42.2 一号解决方案：文件系统检查器(Solution#1:The File System Checker)
早期文件系统采用了很简单的方式解决崩溃一致性。基本上，它们决定让不一致发生然后稍后修复(当重启的时候)。这种懒处理的一个典型例子就是：__fsck[<sup id="content2">2</sup>](#2)__ 工具。`fsck`是一个UNIX工具，用来找到这种不一致然后修复它们；类似的用来检测然后修复硬盘分区的工具在各个系统中都存在。注意，这个工具无法解决所有问题；例如，考虑上面文件系统看起来一致但是inode指向垃圾数据的例子。它们唯一的目标就是确保文件系统元数据内部一致。

`fsck`工具操作几个阶段，在McKusick和Kowalski的论文中总结了。它在文件系统挂载和可用前运行(`fsck`假设在它运行期间没有其它文件系统活动)；一旦它运行完，硬盘文件系统应该处于一致的从而对用户来说可以被访问了。
这里有`fsck`工作的基本总结：
* __超级块(superblock)：__ `fsck`首先检查是否superblock可用，大多数是做合规检查，例如确保文件系统大小打榆已分配数据block数目。这些合规检查的目的就是找到可疑(错误)的superblock；在这种情况下，系统(或者管理员)可能决定使用superblock的替代拷贝。
* __空闲块(free blocks)：__ 下一步，`fsck`扫描inodes，间接blocks，二级间接blocks等等，为了了解文件系统内当前那些block分配了。他使用这些内容产生正确版本的分配位图(allocation bitmap)；因此，如果在位图和inode间有任何不一致，他就是通过信任inode的内容来解决问题(注：就是相信inode内容是很正确的，用来更新位图信息)。同样类型的检验会对所有inode使用，确保它们使用的所有inode都照此在inode位图中被标识了。
* __inode状态：__ 每个inode都检查了有无错误或者其它问题。例如，`fsck`确保了每个被分配inode都有一个有效类型域(例如，普通文件，目录，符号链接等等)。如果inode域有问题就不太容易修复，`fsck`认为这些inode是可疑的并清理掉；相应的位图被更新。
* __inode连接：__ `fsck`还会校验每个已分配inode的链接数。你可以回忆下，链接数指示了有多少个目录包含指向这个特定文件的引用(例如，一个链接)。为了校验链接数，`fsck`扫描整个目录树，从根目录开始，对文件系统内的所有文件和目录构建自己的链接数。如果发现了新计算的结果和inode中的不匹配，就需要采取修正动作，通常是修复inode内的计数。如果一个已分配inode被发现但是没有目录指向它，就把它移动到`lost+found`目录中。
* __重复：__ `fsck`还会检查重复指针，例如两个不同inode指向同一个块的情况。如果一个inode显然是坏的，它就会被清除。要么，被指向的块会被复制，这样每个inode都会有自己的副本。
* __怀块：__ 在扫描所有指针列表时还会执行坏块指针检查。如果一个指针指向了显然超过了它有效范围的块，这个指针就被认为是"坏的"，例如，它的指向块的地址比这个分区地址大。在这种情况下，`fsck`无法做到很智能；他只有从inode或者间接块中删除(清除)这个指针。
* __目录检查：__ `fsck`当然不理解用户文件内容，然而，目录包含了文件系统自身创建的特殊格式化信息。因此，`fsck`对每个目录内容执行了额外的完整性检查，确保`.`和`..`是第一个条目，目录目录中指向的inode都是已分配的，并确保了整个层级中没有目录被链接超过一次。

你可以看到，构建一个可以工作的`fsck`需要对文件系统有极其深入的了解；保证这样的代码可以正确工作非常具有挑战性。然而，`fsck`(以及类似的方式)还有更大，可能更基本的问题：它们太慢了。如果有一个很大的硬盘卷，扫描整个硬盘擦护照所有已分配的块并读取震哥哥目录树可能会花费数十分钟甚至是几小时。`fsck`的性能，随着硬盘容量的增加以及RAID的流行，变得高的不能承受(尽管最近进步了)。

在更高的层级上，`fsck`的基本前提看起来有点不合理。考虑我们上面只有三个块要写入到硬盘的例子；为了修复对三个块中的一个更新发生的问题扫描整个硬盘是极其昂贵的。这类似于，你的要是掉在了卧室地板上，然后执行了 _搜索整个房子找钥匙_ 恢复算法，从地下室开始经过所有房间。他可以工作，但是太浪费了。因此i，随着硬盘(RAID)的增加，研究者和实践者开始找寻其它解决方案。

### 42.3 2号解决方案：日志(或者先写日志) Journaling(or Write-Ahead Logging)
可能解决一致性更新问题最流行的方案是从数据库管理系统窃取到的想法。这个想法，就是 __先写日志(write-ahead loggin)__，被发明专门解决这类问题的。在文件系统中，由于历史原因，我们通常称先写日志为日志(call write-ahead loggin journaling)。第一个这样做的文件系统是Cedar，很多现代系统使用了这个想法，保留Linux ext3和ext4，reiserfs，IBM的JFS，SGI的XFS以及windows的NTFS。

基本想法如下。当更新硬盘，在原地覆写这个结构前，首先写入一点(硬盘的其它地方，一个众所周知的地方)描述了你要做的笔记。写的这些笔记就是"先写"部分，我们把这些写入到我们组织为"日志(log)"的结构中；因此i，先写日志(write-ahead logging)。

通过写入笔记到硬盘中，你就保证了如果在你正在更新的结构被更新(覆写)期间发生了崩溃，你可以回过头查看你做的笔记然后再次尝试；因此，你就会知道崩溃后到底要如何修复(以及如何修复它)，而不用扫描整个硬盘。通过设计，日志在更新期间增加了一些工作量，但是极大的减少了修复期所需工作量。

现在我们会描述 __Linux ext3__，一个流行的日志文件系统，是如何把日志融入到文件系统中的。大多数硬盘数据结构和 __Linux ext2__ 一样，例如，硬盘被分割为blocks组，每个block组包含一个inode位图，数据位图，inodes以及数据块。新的关键结构是日志本身，它只占用了分区内部或者其它设备一小部分空间。因此，一个ext2文件系统(不包含日志)看起来如下：

![show42_3.png "ext2文件系统"](show42_3.png "ext2文件系统")

假设日志被放入在相同的文件系统映像中(尽管有时候它放置在其它设备上，或者是作为文件系统中一个文件)，包含了日志的ext3文件系统看起来如下:

![show42_4.png "ext3文件系统"](show42_4.png "ext3文件系统")

真正的不同就仅仅是日志的存在，当然，还有如何使用它。

#### 数据日志






[<sup id ="1">1</sup>](#content1) 然而，对于用户来说，就是一个问题了，因为他们可能会丢失数据。
[<sup id="2">2</sup>](#conten2)发音要么是"f-s-c-k"要么是"f-s-check"，或者，如果你不喜欢这个工具，"f-suck"也可以。是的，专业人士使用这个术语。