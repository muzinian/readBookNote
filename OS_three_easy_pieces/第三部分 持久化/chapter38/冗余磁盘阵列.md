## 廉价硬盘冗余阵列(Redundant Arrays of Inexpensive Disks RAIDs)
>下文中，没有特殊注明，块对应的英文是block，chunk会加入备注
当我们使用硬盘时，我们有时候希望它会更快一些；I/O操作是很慢的，因此可能会成为整个系统的瓶颈。当我们使用硬盘时，我们有时候希望它更大些；越来越多的数据被放到网上因此我们的磁盘也越来越满。当我们使用硬盘时，我们有时希望它更加可靠；当硬盘失败时，如果我们的数据没有备份，所有有价值的数据都消失了。
>#### 症结：如何制造一个大容量，快速，可靠的硬盘
>我们要怎么制造一个大容量，快速，可靠的存储系统？关键技术是什么？不同方式之间的取舍是什么？

在本章，我们引入了 __廉价硬盘冗余阵列(Redundant Arrays of Inexpensive Disks)__ 以 __RAID__ 著名，这个技术使用多个硬盘串联起来来构建更快，更大以及更可靠的硬盘系统。这个词语在20世纪80年代后期由U.C.伯克利的一组研究员引入(由David Patterson和Randy Katz教授和他们的学生Garth Gibson领导)；大约在同一时间很多不同的研究员同时到达了这个基本想法：使用多个硬盘区构建更好的存储系统。

对外，一个RAID看起来就像一个硬盘：一组可以读写的块。内部，RAID是一个复杂的怪兽，由多个硬盘，内存(包括易失性和非易失性)以及一个或多个处理器组成管理的系统。一个硬件RAID非常类似于一个计算机系统，特定用于管理一组硬盘的任务。

RAIDs比单一硬盘提供了很多优点。一个优点是 _性能_。另一个优点是 _容量_。大量的树需要大量的硬盘。最终，RAIDs可以提升 _可靠性_；扩散数据到多个硬盘(不使用RAID技术)让数据容易遭受到单个硬盘数据丢失；使用某种形式的 __冗余(redundancy)__，RAIDs可以容忍硬盘的一个硬盘的数据丢失帮保持操作没有问题好像没有问题。
>#### tip:透明性保证了部署
>当考虑到如何给一个系统增加新的功能，我们应该总是考虑这种功能是否可以被 __透明__ 的增加，这种方式要求对系统其它部分不做改变。对现存软件做完全的重写(或者彻底的硬件变更)降低了一个想法产生影响的可能。RAID是一个完美的例子，当然它的透明性对它的成功做出了贡献；管理员可以安装一个基于SCSI的RAID存储阵列而不是SCSI硬盘，而系统的其它部分(宿主计算机，OS，等等))并不需要做任何修改就可以使用它。通过解决了 __部署(deployment)__ 的这个问题，RAID从第一天开始就更成功了。

令人惊讶的是，RAID __透明的(transparently)__ 提供了这些优势给那些使用它的系统，例如，RAID对于宿主系统看起来像一个大的硬盘。透明性的美妙是用RAID替换一个硬盘不需要软件做任何修改；操作系统和客户应用不用修改就可以继续操作。以这种方式，透明性极大的提升了RAID的 __部署能力(deployability)__，保证了用户和管理员使用RAID而不用担心软件兼容性。

现在我们讨论RAID几个重要的方面。我们以接口，错误模型开始，然后讨论我们要如何通过三个重要的指标评估(evaluate)RAID的设计：容量，可靠性和性能。然后我们再讨论一些其它对于RAID设计和实现很重要的问题。
### 38.1 接口和RAID内部
对于上面的文件系统，一个RAID看起来像是一个大的，(希望是)快速的和(希望是)可靠的硬盘。就像单个硬盘，它把自己呈现为一个线性块数组，每一个都可以被文件系统(或者其它客户端)读写。

当一个文件系统发起了一个 _逻辑I/O_ 请求到RAID，RAID内部必须要计算为了完成请求，哪个硬盘(哪些硬盘)要被访问，然后发起一个或多个 _物理I/O_ 来完成计算。这些物理I/O精确的本质依赖于RAID的级别，我们将在后面讨论细节。然而，作为一个简单的例子，考虑一个对每个块都由两个拷贝的RAID(每个拷贝都在分离的硬盘)；当写入到这样的 __镜像(mirrored)__ RAID系统，对于发起到它的每一个逻辑I/O，RAID将不得不执行两个物理I/O。

一个RAID系统通常被构建为不同的硬件盒子，加上标准的连接(例如，SCSI 或者 SATA)到宿主上。内部里，RAIDs相当复杂，由一个微控制器(运行着固件指挥RAID的操作)，易失性内存(像是用来缓存读写数据块的DRAM)，在某些情况下，还有非易失性内存(用来安全的缓存写操作和可能的特殊逻辑用来执行部分计算(在某些RAID级别很有用))组成。
### 38.2 错误模型
为了理解RAID并比较各种RAID，我们必须要有一个错误模型。RAIDs被设计为可以检测某种硬盘错误并从中恢复；因此，精确的知道期望什么错误是达成可工作设计的关键点。

我们要假设的第一个错误模型很简单，叫做 __失败停止(fail-stop)__ 错误模型。在这个模型中，只能处于两种状态：工作中或者失败(failed)。工作的硬盘，所有块都可以读写。相反，当硬盘失败了，我们假设它永久的丢失了。

因此，对于现在，我们不需要担心更复杂的"静默"失败例如硬盘腐坏(corruption)。我们也不用担心单个硬盘变得不可工作，因为还有其它工作硬盘(有时候也叫做潜在扇区错误)。我们稍后在考虑这些更复杂的硬盘错误。

### 38.3 如何评估一个RAID
正如我们即将看到的，有很多不同的方式构建一个RAID。每种方式都由不同的特性，这些特性值得评估，用来理解它们的强处和弱点。

具体的说，我们会用三个坐标轴来评估RAID设计。第一个坐标轴是 __容量__；给定包含$B$个块的$N$个硬盘，对于RAID的使用者来说，有多大可用容量？如果没有冗余，答案是 $N\cdot{B}$；相反，如果我们的系统会对每个块保留两份拷贝(叫做 __镜像(mirroring)__)，我们的可用容量就是$N\cdot{B}/2$。不同的模式(例如，基于平等的)会落入这两个之间。

第二个坐标轴是 __可靠性__。给定的设计可以容忍多少块硬盘错误？和我们的错误模型校准，我们假设只有整块硬盘会失败；在后面的章节(例如，在数据完整性)，我们会考虑处理更复杂的失败模式。

最后，第三个坐标轴是 __性能__。性能对评估来说有点挑战性，因为它强依赖于呈现给硬盘阵列的工作集。因此，在评估性能前，我们会先呈现用来测试用的典型工作集。

现在，我们考虑三种重要的RAID设计：RAID level 0(条带striping)，RAID level 1(镜像mirroring)和RAID level 4/5(基于平等的冗余)。每种设计命名为“level”源自于伯克利的Patterson，Gibson和Katz的先驱工作。

### 38.4 RAID Level 0:Striping
第一个RAID等级就是没有一点没有RAID等级，也就是没有冗余。然而，RAID0，或者说更为人所知的 __条带(striping)__ ，提供了卓越的性能和容量上界，所以值得仔细理解。

条带最简单的形式是把块如下(假设有4块硬盘阵列)分布到系统硬盘上(The simplest form of striping will stripe blocks across the disks of the system as follows)))。

![图38_1.png "RAID-0:简单的条带化"](figure38_1.png "RAID-0:简单的条带化")

从图38_1，你可以看到基本想法：以轮询的方式把数组块散布到各个硬盘上。设计为这种方式是为了当请求连续的阵列块(chunks)(例如在一个大的顺序读)时，提议从阵列中榨取最大的并行性。我们把在同一行(a same row)的块叫做一个 __条带__；因此，上图的块0，1，2和3是在同一个条带。

在这个例子里，我们做了简化的假设：在把块放置到下一个硬盘前，每个硬盘只放置一个块(每个块大小是4KB)。但是，这种排列不是必须的。例如，我们可以按照图38_2的方式跨硬盘排列块：

![图38_2 "有着更大块尺寸(Chunk Size)的条带"](figure38_2.png "有着更大块尺寸(Chunk Size)的条带")

在这个例子中，在把块放置到下一个硬盘前，我们在每个硬盘上摆了两个4KB块。因此，这个RAID阵列的 __块大小(chunk size)__ 是8KB，因此一个条带由4个块(chunk)组成，或者是包含32KB的数据。
#### Chunk Sizes
块尺寸(Chunk Size)对阵列的影响最大。例如，小的chunk尺寸暗示着很多文件都会被条列到多个硬盘上，从而增加了读写单个文件的并行度；然而，跨多硬盘访问块的定位时间增加了，因为对整个请求的定位时间是由这个请求多个驱动中最大的定位时间决定的。

另一方面，一个大的chunk尺寸，减少了内部文件的并行度，因此依赖多个并发请求完成高吞吐。然而，大chunk size减少了定位时间；例如，如果单个文件可以放到chunk中因此被放入到单个硬盘中，由于访问文件导致的定位时间就只是单个硬盘的定位时间。

因此，决定“最佳”的chunk size很难，他需要对硬盘系统的工作集要求非常详细的知识。对于这个讨论的剩下部分，我们会假设这个阵列使用单个块的chunk size(4KB)。大多数阵列使用更大的chunk size(例如，64KB)，但是对于我们下面要讨论的问题，具体的chunk size没有关系；因此我们为了简便我们使用单个块作为chunk size。
>#### 附注：RAID映射问题
>在研究RAID的容量，可靠性和性能特性前，我们首先提供一个我们称之为 __映射问题(mapping problem)__ 的附注。这个问题在所有RAID阵列中都存在；简而言之，给定一个对逻辑块的读写，RAID是如何知道访问哪个具体的硬盘和偏移量？
>对于这些简单的RAID等级，我们不需要太多的复杂性就可以正确的映射逻辑块到他们的物理位置。以上面的条带例子为例(chunk size = 1 block = 4KB)。在这种情况下，给定一个逻辑块地址A，RAID可以很容易用下面的两个等式的算出来所需的硬盘以及便宜量：
>$Disk = A \% number\_of\_disks$
>$Offset = A / number\_of\_disks$
>注意，这里都是整数操作(例如，$4/3 = 1$而不是1.3333...)。让我们看看对于一个简单的例子这两个等式是怎么工作的。想象一下在上面第一个RAID中，一个请求到了块14。假设有4块硬盘，这意味着我们感兴趣的硬盘是($14\%4 = 2$)硬盘2。具体的块计算结果是($14/4 = 3$)：块3。因此，块14(注：这里指的是逻辑块14)应该被发现在第三个硬盘(硬盘2，从0开始)的第四个块(块3，从0开始)，这里就是逻辑块14的具体位置。
>你可以想象为了支持不同的chunk size这些等式要怎么修改。试一下，并不难！

#### 回到RAID-0的分析
让我们现在评估一下条带化的容量，可靠性以及性能。从容量的角度来看，他很完美：给定 _N_ 个硬盘，每个硬盘大小是 _B_ 个块。条带提供了$N \cdot B$块的可用容量。从可用性的立场看，条带是很完美的，但是是以很差的方式：任何硬盘失败会导致数据丢失。最后，性能很卓越：所有硬盘都被利用了，通常以并行的方式服务用户I/O请求。
#### 评估RAID性能
在分析fRAID性能时，我们可以考虑两个不同的性能度量。第一个是 _单请求延迟(single-request latency)_。理解单个对RAID的I/O请求延迟很有用因为它揭示了在单个逻辑I/O操作期间可以存在多少个并行度。