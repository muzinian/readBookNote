## 基于锁的并发数据结构
在离开锁之前，我们首先描述一下如何在某些常见数据结构内使用锁。给数据结构增加锁是的数据结构 __线程安全(thread safe)__ 从而让它可以在多线程环境中使用。当然，锁是如何加入进去的决定了这个数据结构的正确性和性能。因此，我们的挑战：
>###症结：如何给数据结构添加锁
>当给出了一个特定的数据结构，我们应该向其添加锁从而让它可以正确工作？更进一步，如何增加锁从而让数据结构可以产出高性能，确保多个线程并发的访问它？

当然，我们很大的压力去覆盖所有数据结构或者所有方法来增加并发性，由于这个主题已经被研究了多年，有数千个研究论文发布。因此，我们希望有效的介绍应该按照哪种思考方式思考，并为你提供良好的素材以供将来的学习。我们发现Moir和Shvit的研究是一个重要的信息源。

###29.1 并发计数器
最简单的一个数据结构是计数器。他用处很广泛并且接口简单。我们定义一个简单的非并发计数器如下：
```C
typedef struct __counter_t
{
    int value;
} counter_t;

void init(counter_t *c)
{
    c->value = 0;
}

void increment(counter_t *c)
{
    c->value++;
}

void decrement(counter_t *c)
{
    c->value--;
}

int get(counter_t *c)
{
    return c->value;
}
```
__没有锁的计数器__

#### 简单但是不具扩展性
正如你看见的，没有同步的计数器就是一个平常的数据结构，只需要很少代码就可以实现。现在我们的挑战是：如何让代码 __线程安全(thread safe)__？下面代码显示了应该如何做：
```C
typedef struct __counter_t
{
    int value;
    pthread_mutex_t lock;
} counter_t;

void init(counter_t *c)
{
    c->value = 0;
    Pthread_mutex_init(&c->lock, NULL);
}

void increment(counter_t *c)
{
    Pthread_mutex_lock(&c->lock);
    c->value++;
    Pthread_mutex_unlock(&c->lock);
}

void decrement(counter_t *c)
{
    Pthread_mutex_lock(&c->lock);
    c->value--;
    Pthread_mutex_unlock(&c->lock);
}

int get(counter_t *c)
{
    Pthread_mutex_lock(&c->lock);
    int rc = c->value;
    Pthread_mutex_unlock(&c->lock);
    return rc;
}
```
__使用了锁的计数器__

这个并发计数器简单且可以正确工作。事实上，它遵循了对于设计最简单和最基本并发数据结构的设计模式：简单的添加一个锁，用来在调用例程时维护数据结构，并在从调用返回时释放锁。按照这种方式，使用 __监视器(monitors)__ 构建数据结构都是类似的，在你调用和返回对象方法时自动请求和获取锁。

现在，你有了一个可以工作的并发数据结构。你可能会遇到性能问题。如果你的数据结构太慢了，需要做的就不止是增加一个锁了；这些优化，如果需要，就是本章剩下来的内容。如果数据结构 _没有_ 太慢，你就不必在做了！没必要在某些事情可以工作的基础上再做花哨的事情。。

为了理解这个简单方式的性能开销，我们运行了一个基准测试：每个线程更新一个共享计数器固定次数；我们调整线程数量观察结果。图29_1显示了活跃线程数从1到4花费的总时间；每个线程更新1百万次计数器。这个实验运行在iMac上，4个Intel 2.7GHz i5CPU，随着CPU的个数增加，我们希望在单位时间有可以做更多的事情。

![图29_1 "传统和近似计数器性能比较"](#figure29_1.png "传统和近似计数器性能比较")

图片中最上面的一条线(标签是"精确")，你可以看到同步计数器的性能扩展性很差。反之，单线程可以在很短的时间内完成一百万的计数(大概是0.03秒)，两个线程并发的更新计数器各一百万次导致了巨大的下滑(耗费了5秒！)。越多线程情况会越糟糕。

理想上，你希望看到多个线程在多处理器上完成速度和单线程在单处理器上一样快。达到这种结果叫做 __完美扩展(prefect scaling)__；尽管有更多工作要做，但是它们是并行完成的，因此完成任务所用时间没有增加。

####可扩展的计数
令人惊讶的是，研究者已经研究了多年如何构建更具扩展性的计数器。更惊讶的是，根据最近在操作系统性能分析上的研究显示，可扩展的计数器很重要；没有可扩展的计数能力，某些运行在Linux系统上的工作会在多核机器上遇到严重的扩展性问题。

为了解决这个问题开发了很多技术。我们将要描述一种叫做 __近似计数器(approximate counter)__。

近似计数器通过多个的 _本地(local)_ 物理计数器(每个处理器一个)，和一个 _全局(global)计数器_ 表示一个逻辑计数器工作(approximate counter works by representing a single logical counter via numerous local physical counters,one per CPU core,as well as a single global counter)。具体说，一个有着4个CPU的机器，会有4个本地计数器和一个全局计数器。除了这些计数器，还有锁：每个本地计数器一个锁[<sup id="content1">1</sup>](#1)，以及一个用于全局计数器的锁。

近似计数的基本想法如下：当在给定CPU上运行的一个线程希望对计数器加1，它会增加自己的本地计数器，访问这些本地计数器是同步的，因为需要响应的本地锁。由于每个CPU都有自己的本地计数器，各个CPU的线程可以在没有竞争的更新自己本地计数器，从而对计数器的更新是可扩展的了。

为了保持全局计数器是最新的(以防线程希望读取它的值)，本地值会周期性发到给全局计数器，通过获取全局锁并按照本地计数器的值增加全局计数器；本地计数器的值被重置为0。

本地到全局传输发生频率由阈值$S$决定。$S$越小，这个计数器的行为表现越像上面的不可扩展计数器；$$$值越大，计数器扩展性越高，但是全局值和实际值的偏差就越大。可以简单的获取全部本地锁和全局锁(按照特定的方式，以避免死锁)来获取精确值，但是这就不可扩展了。

![图29_2 "跟踪近似计数器"](#figure29_2.png "跟踪近似计数器")

为了让这个变得清晰，让我们看一个例子(图figure29_2)。这个例子里面，阈值$S$被设置为$5$，4个CPU上每个线程更新他们自己的本地计数器$L1...L4$。全局计数器值($G$)的变化如图，时间轴方向向下。时间每走一步，可能会增加一次本地计数器；如果本地值到达了阈值$S$，本地值就被发送到全局值然后被重置。

在图29_1中矮的线(标签是"渐进")显示了阈值$S$是1024的近似计数器性能。性能很可观；在4个处理器上更新这个计数器4百万次花费的时间比在单处理器上更新它一百万次高一点点。

图29_3显示了阈值$S$的重要性，图中显示四个线程运行在四个CPU上，在不同阈值下每个线程对这种计数器做一百万次加一操作的时间消耗。如果$S$很低，性能就会很糟糕(但是全局计数就总是接近近似值)；如果$S$很高，性能表现很突出，但是全局值就会滞后(最多是阈值$S$和CPU个数的乘积)。这种精确度/性能权衡就是近似计数器提供的。

![图29_3 "可扩展的近似计数器"](#figure29_3.png "可扩展的近似计数器")

下面是近似计数器的一个大概实现。阅读它，或者更好的是，在某些实验中运行它以获取对它如何工作有更好的理解。
```C
typedef struct __counter_t
{
    int global;                     // global count
    pthread_mutex_t glock;          // global lock
    int local[NUMCPUS];             // local count (per cpu)
    pthread_mutex_t llock[NUMCPUS]; // ... and locks
    int threshold;                  // update frequency
} counter_t;

// init: record threshold, init locks, init values
// of all local counts and global count
void init(counter_t *c, int threshold)
{
    c->threshold = threshold;
    c->global = 0;
    pthread_mutex_init(&c->glock, NULL);
    int i;
    for (i = 0; i < NUMCPUS; i++)
    {
        c->local[i] = 0;
        pthread_mutex_init(&c->llock[i], NULL);
    }
}

// update: usually, just grab local lock and update local amount
// once local count has risen by ’threshold’, grab global
// lock and transfer local values to it
void update(counter_t *c, int threadID, int amt)
{
    int cpu = threadID % NUMCPUS;
    pthread_mutex_lock(&c->llock[cpu]);
    c->local[cpu] += amt; // assumes amt > 0
    if (c->local[cpu] >= c->threshold)
    { // transfer to global
        pthread_mutex_lock(&c->glock);
        c->global += c->local[cpu];
        pthread_mutex_unlock(&c->glock);
        c->local[cpu] = 0;
    }
    pthread_mutex_unlock(&c->llock[cpu]);
}

// get: just return global amount (which may not be perfect)
int get(counter_t *c)
{
    pthread_mutex_lock(&c->glock);
    int val = c->global;
    pthread_mutex_unlock(&c->glock);
    return val; // only approximate!
}
```
__近似计数器实现__

###29.2 并发链表
我们下一个要看的是一个更复杂的结构，链表。让我们再一次以一个基础的方式开始。为了简化，我们忽略一些列表操作而只专注并发插入；我们把查找，删除和其它操作留给读者思考。下面的代码显示了这个基本数据结构。

从代码里你可以看到，代码在进入`insert`例程中简单的获取锁，然后在退出时释放锁。如果`malloc()`碰巧失败会有一个小问题；在这种情况下，带啊吗必须要在失败前释放锁。

```C
// basic node structure
typedef struct __node_t
{
    int key;
    struct __node_t *next;
} node_t;

// basic list structure (one used per list)
typedef struct __list_t
{
    node_t *head;
    pthread_mutex_t lock;
} list_t;

void List_Init(list_t *L)
{
    L->head = NULL;
    pthread_mutex_init(&L->lock, NULL);
}

int List_Insert(list_t *L, int key)
{
    pthread_mutex_lock(&L->lock);
    node_t *new = malloc(sizeof(node_t));
    if (new == NULL)
    {
        perror("malloc");
        pthread_mutex_unlock(&L->lock);
        return -1; // fail
    }
    new->key = key;
    new->next = L->head;
    L->head = new;
    pthread_mutex_unlock(&L->lock);
    return 0; // success
}

int List_Lookup(list_t *L, int key)
{
    pthread_mutex_lock(&L->lock);
    node_t *curr = L->head;
    while (curr)
    {
        if (curr->key == key)
        {
            pthread_mutex_unlock(&L->lock);
            return 0; // success
        }
        curr = curr->next;
    }
    pthread_mutex_unlock(&L->lock);
    return -1; // failure
}
```
__并发链表__


这种异常控制流被认为是容易出错的；最近关于Linux内核补丁的研究发现有很大一部分的bug(将近40%)被发现是存在于这个很少发生的代码路径上(事实上，这种观察触发了我们的一些研究，我们从Linux文件系统中删除了所有的内存失败路径(memory-failing paths)，构建了更加健壮的系统)。

因此，一个挑战：我们可以重写`insert`和`lookup`例程并在保留并发插入正确性的前提下避免再失败路径下添加释放锁的代码么？

在这种情况下，答案是，可以。具体来说，我们可以重新安排代码方式从而让加锁和释放值包围在`insert`代码的实际关键区，这样一个公共退出路径也用于`lookup`代码。前者可以工作是因为`lookup`(这里应该是`insert`?)的部分代码实际上不需要被锁定；假设`malloc()`本身是线程安全的，每个调用它的线程不要关系竟态条件或者其它并发bug。只有在更新共享链表才需要持有锁。修改的代码如下：
```C
void List_Init(list_t *L)
{
    L->head = NULL;
    pthread_mutex_init(&L->lock, NULL);
}

void List_Insert(list_t *L, int key)
{
    // synchronization not needed
    node_t *new = malloc(sizeof(node_t));
    if (new == NULL)
    {
        perror("malloc");
        return;
    }
    new->key = key;

    // just lock critical section
    pthread_mutex_lock(&L->lock);
    new->next = L->head;
    L->head = new;
    pthread_mutex_unlock(&L->lock);
}

int List_Lookup(list_t *L, int key)
{
    int rv = -1;
    pthread_mutex_lock(&L->lock);
    node_t *curr = L->head;
    while (curr)
    {
        if (curr->key == key)
        {
            rv = 0;
            break;
        }
        curr = curr->next;
    }
    pthread_mutex_unlock(&L->lock);
    return rv; // now both success and failure
}
```
__并发链表：重写后__

对于`lookup`例程，就是简单的代码变化：跳出主搜索循环到单一返回路径。这样做再一次减少了代码中锁获取/释放数量，从而减少了在代码中意外引入bug的机会(例如，忘记在返回前释放锁)。

####扩展链表

我们再一次有了基本的并发俩表，也再一次我们又面临它扩展性不是很好这一情况。研究者使用的一个确保链表更高并发性的技术叫做 __手递手锁定(hand-over-hand locking 也叫锁联接 lock coupling)__。

这个想法很简单。不在是整个链表使用一个锁，而是每个节点都有一个锁。当遍历链表时，代码先获取下一个节点的锁然后释放当前的锁(这就是手递手的原因)。

概念上讲，一个手递手链表很有意义；它保证了链表操作具有高并发度。然而，实践上，很难做到让这种结构比单锁方式更快，因为在遍历链表时获取和释放每个节点对应锁的负载是难以承受的。甚至在有一个大链表和大量线程时，通过允许多个同时遍历存在获得到的并发性并不比简单的获取一个锁，执行操作，释放锁要快。可能某种混合方式(每几个节点获取一个锁)更值得调研。

>####Tips:更多的并发度并不是必然的快
>如果你设计的方案会增加很多负担(例如，频繁获取释放锁，而不是只获取释放一次)，事实上是更高的并发可能不重要。简单的方案倾向于能很好的工作，特别是它们很少使用高开销的例程。增加更多的锁和复杂度可能会是你失败的原因。所有这些都说明，只有一种方式可以真正的了解：两种方式都构建(简单但是并发度低，复杂但是并发度高)然后测量它们的行为。在最后，你不能欺骗性能，要么你的想法更快，要么不是。

>####Tips:注意锁和控制流
>一个通用的设计tip，在并发代码和其它地方都有用，就是注意那些会导致函数返回，退出或者其他类似会停止一个函数执行的错误条件发生的控制流。因为很多函数会以请求锁开始，分配内存，或者其它类似有状态的操作，当错误发生，代码不得不在返回前回退所有状态，这是容易导致出现错误的。因此，因此最好以最小化这种模式的方式组织代码。

###29.3 并发队列
现在你知道，总会有标准方法构造并发数据结构：使用一个大的锁。对于队列，我们假设你可以找出这种方法。所以我们跳过这个。

相反，我们将会由Michael和Scott设计的更具并发性的队列。这个数据结构的代码如下：
```C
typedef struct __node_t
{
    int value;
    struct __node_t *next;
} node_t;

typedef struct __queue_t
{
    node_t *head;
    node_t *tail;
    pthread_mutex_t headLock;
    pthread_mutex_t tailLock;
} queue_t;

void Queue_Init(queue_t *q)
{
    node_t *tmp = malloc(sizeof(node_t));
    tmp->next = NULL;
    q->head = q->tail = tmp;
    pthread_mutex_init(&q->headLock, NULL);
    pthread_mutex_init(&q->tailLock, NULL);
}

void Queue_Enqueue(queue_t *q, int value)
{
    node_t *tmp = malloc(sizeof(node_t));
    assert(tmp != NULL);
    tmp->value = value;
    tmp->next = NULL;

    pthread_mutex_lock(&q->tailLock);
    q->tail->next = tmp;
    q->tail = tmp;
    pthread_mutex_unlock(&q->tailLock);
}

int Queue_Dequeue(queue_t *q, int *value)
{
    pthread_mutex_lock(&q->headLock);
    node_t *tmp = q->head;
    node_t *newHead = tmp->next;
    if (newHead == NULL)
    {
        pthread_mutex_unlock(&q->headLock);
        return -1; // queue was empty
    }
    *value = newHead->value;
    q->head = newHead;
    pthread_mutex_unlock(&q->headLock);
    free(tmp);
    return 0;
}
```
__Michael和Scott并发队列__

如果你仔细研究这个代码，你会注意到这里有两个锁，一个用于队头元素，一个用于队尾元素。这两个锁的目标是保证入队和出队操作的并发性。在常见的情况，入队例程只会访问队尾元素的锁，出队例程只会使用队首元素的锁。

Michael和Scott使用的一个技巧是增加一个哑节点(在队列初始化时分配)；这个哑节点确保了头节点和尾节点操作的分离。学习这个代码，或者，敲一下，运行一下，测量一下，深入了解一下它是如何工作的。

队列常常用于多线程应用。然而，这里使用的队列类型(只有锁)通常不能满足这些程序的需求。一个功能更完善的有界队列，这个队列会保证一个线程在队列空或者满时会等待，是我们在下一章条件变量中学些的目标之一。

###29.4 并发hash table
我们以一个简单广泛使用的并发数据结构hashtable结束本章。我们专注于不会扩容的简单hashtable；因为在扩容时需要做一些工作，我们留给了读者。

这个并发hashtable很直接，它使用了我们之前开发过的并发链表，并且工作的非常好。它具有良好性能的原因是，他对每个hash桶(每个桶都是由一个list表示)都是用一个锁而不是对这个hashtable使用单个锁。这样做确保了可以执行更多的并发操作。

```C
#define BUCKETS (101)

typedef struct __hash_t
{
    list_t lists[BUCKETS];
} hash_t;

void Hash_Init(hash_t *H)
{
    int i;
    for (i = 0; i < BUCKETS; i++)
    {
        List_Init(&H->lists[i]);
    }
}

int Hash_Insert(hash_t *H, int key)
{
    int bucket = key % BUCKETS;
    return List_Insert(&H->lists[bucket], key);
}

int Hash_Lookup(hash_t *H, int key)
{
    int bucket = key % BUCKETS;
    return List_Lookup(&H->lists[bucket], key);
}
```

图29_11显示了这个hashtable在并发更新下的性能(每个线程从10000到50000并发更新，一共有4个线程，4CPU的在iMac上)。为了比较，也显示了链表的性能(单个锁)。从图例你可以看到，这个简单的并发hashtable强大的扩展能力，相反，链表就不具有。

![图29_4 "可扩展的hashtable"](#figure29_4.png "可扩展的hashtable")

>####附注：避免过早优化(KNUTH定律)
>当构建并发数据结构时，以最基本方式开始，也就是通过增加一个大锁提供并发访问。通过这样做，你可以构建正确的锁；如果你发现它又性能问题，你可以重新实现它，只改进到他需要的快。正如Knuth说的："过早优化是万恶之源"。
>很多操作系统在开始过渡到多处理器时都是利用了单一锁，包括Sun OS和Linux。稍后，这个锁有了自己的名字，__big kernel lock(BKL)__。在很多年里，这个简单的方式是很好的一个，但是当duoCPU系统变得常见，在内核中一个时刻只允许一个活跃线程成为了性能瓶颈。因此，这是最后的时间为了提高这些系统并发性而进行优化。在Linux内，很多直接方式被使用：使用多个锁替换一个锁。在Sun内，使用了更彻底的方案：构建一个全新的操作系统，也就是Solaris，它从第一天就让并发性变得更基本。阅读Linux和Solaris内核书了解这些系统更多细节。

###29.5 总结
我们已经介绍了几个并发数据结构的例子，从计数器，到列表和队列，到十分常见并被深度使用的hashtable。在这期间我们已经学到了几个重要课程：在控制流改变的附近仔细管理锁的获取和释放；提供更高并发度并不是必然提高性能；性能问题只有在存在时才解决。最后一点，避免 __过早优化(premature optimization)__，对任何牢记性能开发人员的中心；如果不能很好的提高整体应用性能就没有必要让事情运行的更快。

当然我们只是看到高性能结构的表面。看看Moir和Shavit关于这些内容更优质的总结。具体的，你可能对其它结构感兴趣(例如B-tree)；对这写知识，数据库课程是很好的一个方向。你可能对那些不使用传统锁的技术感兴趣；例如 __非阻塞数据结构__ ，我们将在常见并发bug中一品它的味道，但是通常这个主题涉及到一整块需要比本书提供的更多研究知识。